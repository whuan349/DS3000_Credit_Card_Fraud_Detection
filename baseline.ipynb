{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7C0qtbNDDjCwgza+rD6wH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/whuan349/DS3000_Credit_Card_Fraud_Detection/blob/master/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1 — Metrics Foundations**\n",
        "\n",
        "This Task:\n",
        "\n",
        "*   Implements a trivial baseline classifier that predicts all transactions aS legitimate (Class = 0).\n",
        "\n",
        "*   Computes key evaluation metrics (Accuracy, Precision, Recall, F1-Score) to establish a starting performance benchmark.\n",
        "\n",
        "*   Demonstrates why accuracy alone is misleading for fraud detection by showing that minority-class metrics remain zero when no fraud cases are detected.\n",
        "*   Highlights the necessity of using precision, recall, and F1-score to properly assess model performance on imbalanced classification tasks.\n",
        "\n",
        "*   Provides a reference point against which all subsequent machine-learning models are compared."
      ],
      "metadata": {
        "id": "4JC44yL1q543"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MC41xQgQSTk5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and inspect the data\n",
        "df = pd.read_csv(\"creditcard_2023.csv\")\n",
        "\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJOOD51bTE-U",
        "outputId": "ccc1d939-1a6f-4c2b-f0a2-0f9d15560087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   id        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0 -0.260648 -0.469648  2.496266 -0.083724  0.129681  0.732898  0.519014   \n",
            "1   1  0.985100 -0.356045  0.558056 -0.429654  0.277140  0.428605  0.406466   \n",
            "2   2 -0.260272 -0.949385  1.728538 -0.457986  0.074062  1.419481  0.743511   \n",
            "3   3 -0.152152 -0.508959  1.746840 -1.090178  0.249486  1.143312  0.518269   \n",
            "4   4 -0.206820 -0.165280  1.527053 -0.448293  0.106125  0.530549  0.658849   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0 -0.130006  0.727159  ... -0.110552  0.217606 -0.134794  0.165959  0.126280   \n",
            "1 -0.133118  0.347452  ... -0.194936 -0.605761  0.079469 -0.577395  0.190090   \n",
            "2 -0.095576 -0.261297  ... -0.005020  0.702906  0.945045 -1.154666 -0.605564   \n",
            "3 -0.065130 -0.205698  ... -0.146927 -0.038212 -0.214048 -1.893131  1.003963   \n",
            "4 -0.212660  1.049921  ... -0.106984  0.729727 -0.161666  0.312561 -0.414116   \n",
            "\n",
            "        V26       V27       V28    Amount  Class  \n",
            "0 -0.434824 -0.081230 -0.151045  17982.10      0  \n",
            "1  0.296503 -0.248052 -0.064512   6531.37      0  \n",
            "2 -0.312895 -0.300258 -0.244718   2513.54      0  \n",
            "3 -0.515950 -0.165316  0.048424   5384.44      0  \n",
            "4  1.071126  0.023712  0.419117  14278.97      0  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_8kh10nTG9c",
        "outputId": "fd283003-1f5a-4e18-80f7-a0274f8e770c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
            "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
            "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
            "       'Class'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Clean and prepare the Class column (target variable)\n",
        "df = df.dropna(subset=[\"Class\"])        # Drop any rows where the label (Class) is missing\n",
        "df[\"Class\"] = df[\"Class\"].astype(int)   # Ensure the label is stored as integer 0/1\n",
        "\n",
        "# Check distribution of the labels\n",
        "print(df[\"Class\"].value_counts())\n",
        "print(\"\\nClass distribution (proportion):\")\n",
        "print(df[\"Class\"].value_counts(normalize=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nU5jCWAa3YI",
        "outputId": "27d1195c-433e-4b07-c786-59723c7dba8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class\n",
            "0    284315\n",
            "1    284315\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class distribution (proportion):\n",
            "Class\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create baseline predictions\n",
        "y_true = df[\"Class\"].values\n",
        "y_pred = np.zeros_like(y_true)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# 4. Compute evaluation metrics\n",
        "accuracy = (y_pred == y_true).mean()\n",
        "precision = precision_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "recall = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "\n",
        "print(\"==== BASELINE MODEL RESULTS ====\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcEwHrTGa6VO",
        "outputId": "123adbc1-314b-4f34-f9ae-b12143777eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== BASELINE MODEL RESULTS ====\n",
            "Accuracy: 0.5\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1 Score: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of Zero Precision, Recall, and F1-Score\n",
        "\n",
        "The baseline classifier was designed to predict all transactions as legitimate (Class = 0). As a result, the model never produced any predictions for the fraud class (Class = 1). Because no transactions were predicted as fraudulent, the number of true positives was zero.\n",
        "\n",
        "Precision measures the proportion of predicted fraud cases that are actually fraud; since the baseline predicted zero fraud cases, precision is zero. Recall measures the proportion of actual fraud cases that were correctly identified; again, because no fraud cases were predicted, recall is zero. The F1-score, which is the harmonic mean of precision and recall, is also zero because both values are zero.\n",
        "\n",
        "These results demonstrate that although the baseline classifier may achieve moderate or even high accuracy depending on class distribution, it completely fails to perform the primary task of interest—detecting fraudulent transactions—highlighting the need for more advanced machine learning models and more appropriate evaluation metrics."
      ],
      "metadata": {
        "id": "YnbMjjepsCD-"
      }
    }
  ]
}